{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "# DL HW2\n",
        " - Pretrained Models\n",
        "\n",
        "In this assignment, you will analyze and compare a few pre-trained models from the field of computer vision.\n",
        "\n",
        "The assignment is divided into several tasks:\n",
        "\n",
        "- **EX1 - The Imagenette Dataset (10 pts)**  \n",
        "- **EX2 - Pretrained Models (20 pts)**  \n",
        "- **EX3 - Visualizing Feature Maps (30 pts)**  \n",
        "- **EX4 - K-Nearest Neighbors (KNN) in the Embedding Space (50 pts)**  \n",
        "\n",
        "Note: In this assignment, you will not train any models or aim to reach specific accuracy levels. Instead, you will investigate the behavior of a few pre-trained models.\n",
        "\n",
        "---\n",
        "\n",
        "## Grading\n",
        "\n",
        "The grading for each section is indicated in the title. Grading will be based on the following criteria:\n",
        "\n",
        "- **Following Instructions**  \n",
        "- **Presentation**: Clear figures (with labels, titles, etc.), well-written discussions and comments, organized notebook, no leftover debugging prints, and no redundant functions.  \n",
        "- **Clear and Efficient Code**: Your code should be clear and neat. Write clear comments and avoid `for loops` when vectorized operations are available. Use the GPU when possible.  \n",
        "- **Discussion**: Ensure you write a discussion whenever it is required.  \n",
        "\n",
        "---\n",
        "\n",
        "Submit the fully executed notebook.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "WN-7dc8JIa_F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EX1 - Imagenette (Small subset of ImageNet) Dataset (10pts)\n",
        "\n",
        "\n",
        "\n",
        "1.   Download the ```Imagenette``` dataset from ```torchvision``` ([link](https://pytorch.org/vision/0.19/generated/torchvision.datasets.Imagenette.html#torchvision.datasets.Imagenette)). Make sure the set size=\"160px\" to avoid long downloading time.\n",
        "2. Preprocess the data - resize to 256x256 and take a central crop of size 224. ToTensor, Normalize and so on.\n",
        "3. Describe the dataset: number of samples for each set, classes labels, classes labels distribtuion (are they balanced?)\n",
        "\n",
        "4. Plot 5 random samples from each class a present them in a ```num_classes X 5``` (rows X columns) grid.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "W_gp7AOH8rpa"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7h7QpwUZIqcN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EX2 - Pretrained models (20pts)\n",
        "\n",
        "1. Choose 2 models from ```torchvision.models``` ([link](https://pytorch.org/vision/stable/models.html)) that were pretrained on ImageNet. The third model is one of ResNet18/34/50 (the other models can't other ResNet variation).\n",
        "2. Use ```torch-summary``` to summarize each model for an input of shape ```1x3x224x224```.\n",
        "3. Describe each model in 2-4 lines. Think carefully what information might be relevant for this homework assignment (see EX3-EX4). It is recommended to read the paper in which each model was first presented but it is not required. You are, however, required to go over model's description in torchvision documentation (see the information tab for ResNet18 for an example [link](https://pytorch.org/vision/0.19/models/generated/torchvision.models.resnet18.html#torchvision.models.resnet18) )\n",
        "4. Summarize the similarities and difference between the 3 models."
      ],
      "metadata": {
        "id": "yDdQGME1-Swl"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SNuxIeqY7hnN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EX3 - Visualizing feature maps (30pts)\n",
        "In this section you will visualize the feature maps learned by your chosen models.\n",
        "\n",
        "\n",
        "*   In a convoultion-based models (i.e., ResNet, ConvNext), use the final conv block. If the image resolution is too small do to maxpooling operations, you may choose a different block.\n",
        "*   In a vision transformer (ViT), use model output without the cls token (if relevant).\n",
        "\n",
        "Assignments:\n",
        "\n",
        "\n",
        "1.   Describe the feature map shape for an input image of shape ```1x3x224x224``` (i.e., ```1x512xHxW``` for some model).\n",
        "2. Choose 1 class from Imagenette and sample 10 random images.\n",
        "3. Extract the feature maps from each model and perform PCA on the channel dim and reduce it to 3. The output should be of size ```10x3xHxW```\n",
        "4. Resize the features to 112x112 and plot the images for each model (3 row x 10 columns)\n",
        "5. Summarize the similarities and difference between the 3 models according to the PCA for the features you have presented.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7CbWbGNf_Kbn"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JEXI-OMUB7lp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EX4 - K-Nearest Neighbors (KNN) in the Embedding Space (50pts)\n",
        "Also know ans \"few-shot/ Zero-shot learning\".\n",
        "Perform KNN classification using the feature vectors from each of the 3 models between the train and test set of Imagenette.\n",
        "\n",
        "\n",
        "## Visualizing the embedding layer distribution.\n",
        "* Write an ```extract_embedding(dataloader, model, model_name)``` function that takes in a dataloader and a model and return the ```N x Num_features``` matrix for that set. Make sure that shuffle is off.\n",
        "* You may write different condition for different models. I.e., the embedding in a ViT model is the ```CLS``` token.\n",
        "\n",
        "* Plot the t-SNE of the train set for each of the 3 models. Color label the points according to the class labels (see t-SNE implementation by sklearn).\n",
        "\n",
        "* Write a 2-3 lines disscussion about the data distribution of each model and their comparison.\n",
        "\n",
        "\n",
        "## Build a KNN ```class``` with the following methods:\n",
        "\n",
        "\n",
        "*   ```init()```: takes the num_classes and other useful information.\n",
        "* ```extract_embedding(X, model)```: Extracts the embedding vector for the entire train set using the model. X_features should be ```Nxnum_ft```. Call the function from the previous section.\n",
        "\n",
        "*   ```fit(X_train, y_train, model)```: Extracts the embedding vector for the entire train set using the ```extract_embedding()``` method and stores it as ```self.X_train_ft``` and ```self.y_labels``` (the labels from the train set).  \n",
        "\n",
        "\n",
        "*   ```predict(X_test, model, n_neighbors)```: Extracts the embedding vector for the entire train set using the ```extract_embedding()``` method and performs KNN for a given K. Returns the predicted_labels.\n",
        "* ```compute_accuracy(y_true, y_pred)```.\n",
        "\n",
        "\n",
        "## Compare the KNN accuracy\n",
        "using each of the 3 models for $K\\in[1,3,5 ]$ and report the results in a table or a graph. Summarize the results in 2-4 lines.\n"
      ],
      "metadata": {
        "id": "VPCKV-Jk_Kd4"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kT3GVCx9CHtB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}